{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZpRCFYVKuDV"
   },
   "source": [
    "# IN4080: obligatory assignment 2 (Autumn 2025)\n",
    "\n",
    "Mandatory assignment 2 consists of three parts. In Part 1, you will experiment with a greedy sequence labeling model and investigate the impact of different feature types on **part-of-speech tagging** performance (9 points + 2 optional bonus points). In Part 2, you will evaluate the best model on a different task, **named entity recognition** (5 points). In Part 3, you will return to the **text classification task** and implement a simple feed-forward neural network for it (6 points).\n",
    "\n",
    "You should answer all three parts. You are required to get at least 12/20 points to pass. The most important is that you try to answer each question (possibly with some mistakes), to help you gain a better and more concrete understanding of the topics covered during the lectures. In the first part, there are also bonus questions for those of you who would like to deepen their understanding of the topics covered by this assignment.\n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions.\n",
    "- You may redeliver in Devilry before the deadline (__Friday, October 3 at 23:59__), but include all files in the last delivery.\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the files already provided for this assignment.\n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory\\_2_\n",
    "- You can work on this assignment either on the IFI machines or on your own computer.\n",
    "\n",
    "*The preferred format for the assignment is a completed version of this Jupyter notebook*, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have done and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks. If you are using code editors such as VS Code, you should make sure that code completion plugins such as Copilot are disabled when working on the assignment.\n",
    "\n",
    "In this assignment, we'll use the following Python modules: `scikit-learn, pyconll, matplotlib, sentence_transformers`. Make sure you have installed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u52Zo7MeKuDX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (1.7.2)\n",
      "Collecting pyconll\n",
      "  Downloading pyconll-3.2.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (3.10.0)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from sentence_transformers) (0.34.4)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.17.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/in4080_2025/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading pyconll-3.2.0-py3-none-any.whl (27 kB)\n",
      "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyconll, tokenizers, transformers, sentence_transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.0\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.0:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.0\n",
      "\u001b[2K  Attempting uninstall: transformersm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: transformers 2.1.1━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling transformers-2.1.1:━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [tokenizers]\n",
      "\u001b[2K      Successfully uninstalled transformers-2.1.1━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [tokenizers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [sentence_transformers]ence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pyconll-3.2.0 sentence_transformers-5.1.1 tokenizers-0.22.1 transformers-4.56.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# if you use a virtual environment, you can install the modules like this\n",
    "%pip install scikit-learn pyconll matplotlib sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHHxXyhuORqw"
   },
   "source": [
    "If you use Fox, make sure to load the following modules (we will not use `pyconll` in this case):\n",
    "\n",
    "* nlpl-sentence-transformers/3.1.1-foss-2022b-Python-3.10.8\n",
    "* nlpl-transformers/4.47.1-foss-2022b-Python-3.10.8\n",
    "* nlpl-llmtools/06-foss-2022b-Python-3.10.8\n",
    "* nlpl-nlptools/04-foss-2022b-Python-3.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0maKnelKuDY"
   },
   "source": [
    "## Part 1 – Greedy logistic regression taggers and feature engineering\n",
    "\n",
    "In the lecture, we have discussed Matthew Honnibal's proposal of a discriminative sequence labelling model with greedy decoding. He argued that an extended set of features is more helpful for tagging than exact (Viterbi) decoding. Therefore, we skip HMMs and the Viterbi algorithm here and focus on models based on logistic regression.\n",
    "\n",
    "Scikit-learn doesn't contain implementations of sequence labeling models. Therefore, we provide you with some basic code. The code below defines a `GreedyTagger` with a `fit()` function for training and a `predict()` function for prediction/testing. By default, it uses a standard Scikit-learn logistic regression classifier under the hood, which takes a feature vector for a word (and its context) as input and provides the most likely class label as output.\n",
    "\n",
    "Your task in this part will mainly consist in defining how a word (and its context) is converted to a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DVKaMq-fKuDY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class GreedyTagger:\n",
    "    \"\"\"\n",
    "    Tagger based on logistic regression or any other classification algorithm supported by Scikit-Learn.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_function,\n",
    "        classifier=LogisticRegression(solver=\"saga\", max_iter=500, tol=0.001),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a GreedyTagger object.\n",
    "\n",
    "        Args:\n",
    "            feature_function: A function that transforms the input sequence and the index of the current word into a set of key-value pairs.\n",
    "            classifier: A Scikit-Learn classifier instance.\n",
    "        \"\"\"\n",
    "        self.features = feature_function\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        self.label_ids = {}  # mapping from labels to numeric label ids\n",
    "        self.id_labels = {}  # mapping from numeric label ids to labels\n",
    "\n",
    "    def fit(self, dataset_X, dataset_Y):\n",
    "        \"\"\"\n",
    "        Trains the tagger. Creates a list of inputs (feature vectors of individual words) and a list of labels (numeric ids) and calls the `fit` function of the base classifier on these lists.\n",
    "\n",
    "        Args:\n",
    "            dataset_X: The input training data, formatted as a list of lists. Each list item represents a sentence and corresponds to a list of tokens.\n",
    "            dataset_Y: The training data labels, formatted as a list of lists. Each list item represents a sentence and corresponds to a list of labels (e.g. POS-tags or BIO-tags).\n",
    "        \"\"\"\n",
    "        flattened_dataset_X = []  # a flat list of training instances\n",
    "        flattened_dataset_Y = []  # a flat list of label ids for the training instances\n",
    "        for sentence_X, sentence_Y in zip(dataset_X, dataset_Y):\n",
    "            # sentence_X is a list of words\n",
    "            # sentence_Y is a list of labels, one for each word of the sentence\n",
    "            history = []\n",
    "            for i, (x, y) in enumerate(zip(sentence_X, sentence_Y)):\n",
    "                feature_dict = self.features(sentence_X, i, history)\n",
    "                flattened_dataset_X.append(feature_dict)\n",
    "                if y not in self.label_ids:\n",
    "                    self.label_ids[y] = len(self.label_ids)\n",
    "                flattened_dataset_Y.append(self.label_ids[y])\n",
    "                history.append(y)\n",
    "        transformed_dataset_X = self.vectorizer.fit_transform(flattened_dataset_X)\n",
    "        transformed_dataset_Y = np.array(flattened_dataset_Y)\n",
    "        self.id_labels = {self.label_ids[label]: label for label in self.label_ids}\n",
    "        self.classifier.fit(transformed_dataset_X, transformed_dataset_Y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset_X):\n",
    "        \"\"\"\n",
    "        Predicts the labels for the input data, sentence by sentence.\n",
    "\n",
    "        Args:\n",
    "            dataset_X: The input training data, formatted as a list of lists. Each list item represents a sentence and corresponds to a list of tokens.\n",
    "\n",
    "        Returns:\n",
    "            list: The predicted labels, in the same format as dataset_X (list of lists).\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for sentence_X in dataset_X:\n",
    "            flattened_sentence_X = []\n",
    "            history = []\n",
    "            for i, word in enumerate(sentence_X):\n",
    "                feature_dict = self.features(sentence_X, i, history)\n",
    "                flattened_sentence_X.append(feature_dict)\n",
    "            transformed_sentence_X = self.vectorizer.transform(flattened_sentence_X)\n",
    "            predicted_sentence_Y = self.classifier.predict(transformed_sentence_X)\n",
    "            predicted_labels_Y = [self.id_labels[y] for y in predicted_sentence_Y]\n",
    "            predictions.append(predicted_labels_Y)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzU7zujcKuDZ"
   },
   "source": [
    "When instantiating this class, you need to provide a so-called *feature function* that defines how the feature vector is created. In the provided implementation, this is done in two steps:\n",
    "1. The feature function creates a feature dictionary. Each key of the dictionary defines a one-hot vector, and the value determines which value of the vector will be set to one. For example, the feature dictionary `{\"curr_word\": \"love\", \"prev_word\": \"I\", \"next_word\": \"fish\"}` defines three one-hot vectors, which together represent the word `love` in the sequence `I love fish`. The keys can be chosen freely.\n",
    "2. The `DictVectorizer` class of Scikit-Learn will convert the feature dictionary to actual one-hot vectors and concatenate them into a single vector. This step is already taken care of in the `fit()` function.\n",
    "\n",
    "A basic feature function that only uses the current word could look like this (we will use the parameter `history` later, but don't worry about it now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KQ57yX2KuDZ"
   },
   "outputs": [],
   "source": [
    "def basic_features(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4XnuJi9KuDZ"
   },
   "source": [
    "The `predict()` function only returns the predicted labels, it doesn't compare them to the ground truth. Here is another function that computes accuracy for the predictions of a dataset. However, the function is incomplete -- it only uses the first sentence of the dataset.\n",
    "\n",
    "**Task 1.1** (1 point): Modify the function to take into account all sentences of the dataset. The easiest way to achieve this is to flatten a list of lists into a single list, so you'll only have to call the `accuracy_score` function once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8oAxHDB6KuDZ"
   },
   "outputs": [],
   "source": [
    "def tagging_accuracy(dataset_Y_true, dataset_Y_pred):\n",
    "    ## CHANGE CODE HERE\n",
    "    # flatten a list of lists \n",
    "    words_Y_true = [y for sentence in dataset_Y_true for y in sentence]\n",
    "    words_Y_pred = [y for sentence in dataset_Y_pred for y in sentence]\n",
    "    assert len(words_Y_true) == len(words_Y_pred) # check if they have the same length \n",
    "    acc = sklearn.metrics.accuracy_score(words_Y_true, words_Y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C8F7gNSKuDa"
   },
   "source": [
    "Finally, we need some actual data to work with. We provide you with part-of-speech annotated data for Norwegian Bokmål taken from (https://universaldependencies.org/).\n",
    "\n",
    "There are two ways of loading the data here:\n",
    "\n",
    "1. You can use the `pyconll` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "klqhhI5vKuDa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15696 sentences loaded from file no_bokmaal-ud-train.conllu\n",
      "2409 sentences loaded from file no_bokmaal-ud-dev.conllu\n",
      "1939 sentences loaded from file no_bokmaal-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "\n",
    "\n",
    "def load_data_from_conllu(filename):\n",
    "    data = pyconll.load_from_file(filename)\n",
    "    X, Y = [], []\n",
    "    for sentence in data:\n",
    "        X.append(\n",
    "            [token.form for token in sentence]\n",
    "        )  # the \"form\" field contains the words\n",
    "        Y.append(\n",
    "            [token.upos for token in sentence]\n",
    "        )  # the \"upos\" field contains the part-of-speech tags in universal POS format\n",
    "    print(f\"{len(X)} sentences loaded from file {filename}\")\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "train_data_x, train_data_y = load_data_from_conllu(\"no_bokmaal-ud-train.conllu\")\n",
    "valid_data_x, valid_data_y = load_data_from_conllu(\"no_bokmaal-ud-dev.conllu\")\n",
    "test_data_x, test_data_y = load_data_from_conllu(\"no_bokmaal-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4Tkfmw2Pzm6"
   },
   "source": [
    "2. You can use our preprocessed `.conllu` files, which are saved as `.json` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RY-QXBCnNQ9H"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def load_data_from_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    X, Y = [], []\n",
    "    for sentence in data:\n",
    "        X.append(sentence[\"sentence\"])\n",
    "        Y.append(sentence[\"labels\"])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "train_data_x, train_data_y = load_data_from_json(\"no_bokmaal-ud-train.json\")\n",
    "valid_data_x, valid_data_y = load_data_from_json(\"no_bokmaal-ud-dev.json\")\n",
    "test_data_x, test_data_y = load_data_from_json(\"no_bokmaal-ud-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI5WP0LXQcDg"
   },
   "source": [
    "In any of the options, the data should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0OIf6suNFUa",
    "outputId": "d5bbed55-34e9-4624-fb73-ecc533be67bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Lam', 'og', 'piggvar', 'på', 'bryllupsmenyen'],\n",
       " ['NOUN', 'CCONJ', 'NOUN', 'ADP', 'NOUN'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x[0], train_data_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DJCCAXuKuDa"
   },
   "source": [
    "Now, we can put everything together. Let us train a tagger on the training data and evaluate it on the validation set.\n",
    "\n",
    "**Task 1.2** (1 point): Write the corresponding code and output the accuracy (as a number between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BujfCc_SKuDa"
   },
   "outputs": [],
   "source": [
    "# instantiate the tagger\n",
    "def basic_features(sentence,i, histoty):\n",
    "    return {\"current_word\": sentence[i]}\n",
    "\n",
    "tagger = GreedyTagger(feature_function=basic_features)\n",
    "\n",
    "# train the tagger on the training data\n",
    "tagger.fit(train_data_x, train_data_y)\n",
    "\n",
    "#predit on the validation set\n",
    "pred_validation = tagger.predict(valid_data_x)\n",
    "\n",
    "#computing the accuracy \n",
    "acc=tagging_accuracy(valid_data_y,pred_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOUnlGmhKuDb"
   },
   "source": [
    "**Task 1.3** (1 point): The basic feature function only looks at the current word. Also add the previous word, the next word, and the word before the previous one. Make sure to handle edge cases at the beginning and at the end of the sentence. For example, the feature dictionary for the first word of `I love fish` could look as follows: `{\"curr_word\": \"I\", \"prev_word\": \"<START>\", \"prev2_word\": \"<START>\", \"next_word\": \"love\"}` (you should still not need the parameter `history` for this). Add one feature at a time, train a model, and record the accuracy. Describe which combination of features works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mrmHJVpHKuDb"
   },
   "outputs": [],
   "source": [
    "def context_features(sentence, i, history):\n",
    "    #define the current, previous, next and the one before previous\n",
    "    curr = sentence[i]\n",
    "    prev = sentence[i-1] if i-1 >= 0 else \"<START>\"\n",
    "    prev2 = sentence[i-2] if i-2 >= 0 else \"<START>\"\n",
    "    nextw = sentence[i+1] if i+1 < len(sentence) else \"<END>\"\n",
    "\n",
    "    features = {\n",
    "        \"curr_word\": curr,\n",
    "        \"prev_word\": prev,\n",
    "        \"prev2_word\": prev2,\n",
    "        \"next_word\": nextw\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2UwoPnFKuDb"
   },
   "source": [
    "**Task 1.4** (1 point): Up to now, we are still using a unigram model, in the sense that no information about the *labels* at previous positions is incorporated. Let us change this now by adding so-called *transition features*. Add a key `prev_tag` to the feature function and fill it with the label of the previous position. You can use the `history` parameter for this. Write your new feature function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3Y3ACCbXKuDc"
   },
   "outputs": [],
   "source": [
    "def transition_features(sentence, i, history):\n",
    "    #define the current, previous, next and the one before previous\n",
    "    curr = sentence[i]\n",
    "    prev = sentence[i-1] if i-1 >= 0 else \"<START>\"\n",
    "    prev2 = sentence[i-2] if i-2 >= 0 else \"<START>\"\n",
    "    nextw = sentence[i+1] if i+1 < len(sentence) else \"<END>\"\n",
    "\n",
    "    #incorporating previous tag\n",
    "    prev_tag = history[-1] if len(history) > 0 else \"<START>\"\n",
    "    \n",
    "    features = {\n",
    "        \"curr_word\": curr,\n",
    "        \"prev_word\": prev,\n",
    "        \"prev2_word\": prev2,\n",
    "        \"next_word\": nextw,\n",
    "        \"prev_tag\": prev_tag\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjHp2_sVKuDc"
   },
   "source": [
    "Before you start training the model with the transition features, have a closer look at the starter code provided at the beginning. You will notice that the `history` parameter is correctly filled in the `fit()` function, but that it always remains empty in the `predict()` function. With this setup, adding transition features will not have any impact on the accuracy. You will therefore have to implement an alternative prediction function that fixes this issue.\n",
    "\n",
    "**Task 1.5** (2 points): Complete the `predict_with_history()` function below such that the history list is filled with the predicted label at each step. Instead of calling `self.classifier.predict()` once per sentence, you will have to call it for each word separately, because the feature vector for a given position can only be computed once the previous word has been labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SllsqieoKuDc"
   },
   "outputs": [],
   "source": [
    "def predict_with_history(self, dataset_X):\n",
    "    predictions = [] # one list of tags per sentence \n",
    "    for sentence_X in dataset_X: # loop over the sentences\n",
    "        history = [] # stores the predicted labels for the current sentence\n",
    "        sent_preds = [] # save the rpedicted labels for the current sentence\n",
    "        for i, _ in enumerate(sentence_X): # from left to the right of the sentence toke by token\n",
    "            feature = self.features(sentence_X, i, history) # build the feature dictionary for token i where the history is incorporated\n",
    "            X_i = self.vectorizer.transform([feature]) # convert the dictionary to vector\n",
    "            y_id = self.classifier.predict(X_i)[0] # using the classifier to predict id\n",
    "            y = self.id_labels[y_id] # map the id back to the label\n",
    "            sent_preds.append(y) # save the prediction for this token \n",
    "            history.append(y) # update the history\n",
    "        predictions.append(sent_preds)\n",
    "    return predictions\n",
    "\n",
    "# attach the function to the GreedyTagger class\n",
    "setattr(GreedyTagger, \"predict_with_history\", predict_with_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOQUZH70KuDc"
   },
   "source": [
    "**Task 1.6** (1 point): Train a new tagger using the transition features and the `predict_with_history()` function. Do the transition features help? Note: The prediction will be significantly slower than before because of the modifications made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgnwpCBoKuDc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCJWsOSJKuDd"
   },
   "source": [
    "**Task 1.7** (_optional, 2 extra points_): You may have noticed that there is a significant implementation difference between the `fit()` and `predict_with_history()` functions. During training, the `history` list is filled with the *gold* labels directly taken from the annotated training data. During prediction, the gold labels are not available, so we have to resort to the *predicted* labels. Could this approach lead to any problems? If so, how could it be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifeOutywKuDd"
   },
   "source": [
    "**Task 1.8** (_optional, 2 extra points_): Take the currently best-performing feature function and add even more features to get an even better tagger. Some ideas:\n",
    "- Extract suffixes and prefixes of $n$ characters ($1 \\leq n \\leq 4$) from the current, previous or next word.\n",
    "- Is the current word a number?\n",
    "- Is it capitalized?\n",
    "- Does it contain capitals?\n",
    "- Does it contain a hyphen? etc.\n",
    "\n",
    "What is the best feature set you can come up with? Define various feature sets and select the best one based on validation set accuracy.\n",
    "If you use sources for finding tips about good features (like articles, web pages, etc.) make references to the sources and explain what you got from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntvS0rwsKuDd"
   },
   "source": [
    "Up to now, we have been working on the development set. It is time now to evaluate your best-performing model on the held-out test set.\n",
    "\n",
    "**Task 1.9** (2 points): First, compute the *test set accuracy* of your best model. However, the accuracy only gives us a high-level overview of the performance of a tagger, but we may be interested in finding out more details about where the tagger makes the mistakes. The universal tagset is reasonably small, so we can produce a *confusion matrix*. Take a look at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html and generate a confusion matrix for the results. Which pairs of tags are most easily confounded? You can find the documentation of the tagset in the following link: https://universaldependencies.org/u/pos/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoW2pls_KuDd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy3Eh-0bKuDd"
   },
   "source": [
    "## Part 2 – Span identification with sequence models\n",
    "\n",
    "In this part, you'll use the same sequence models for a different task: named entity recognition for English.\n",
    "\n",
    "**Task 2.1** (0.5 point): Write the code to load the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iR2IZXHFKuDd"
   },
   "outputs": [],
   "source": [
    "def load_ner_data(filename):\n",
    "    # IMPLEMENT THE CODE TO LOAD THE NER DATA\n",
    "\n",
    "train_nerdata_x, train_nerdata_y = load_ner_data('ner-train.txt')\n",
    "test_nerdata_x, test_nerdata_y = load_ner_data('ner-test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIPlLkxiKuDd"
   },
   "source": [
    "**Task 2.2** (0.5 point): Train a greedy tagger with the NER training data. Use any feature set that worked well for POS tagging, but *do not include transition features* for now. Compute the token-level accuracy of the test set, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTmQTrVWKuDd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omgTVrLUKuDd"
   },
   "source": [
    "**Task 2.3** (1 point): Token-level accuracy is not well adapted to span identification tasks (why?). Instead, we want to compute precision, recall and f-score of the *entities*. Adapt the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnXzNZmiKuDd"
   },
   "outputs": [],
   "source": [
    "def get_ranges(l):\n",
    "    \"\"\"\n",
    "    Helper procedure for eval_ner.\n",
    "    You're not expected to change anything here.\n",
    "    \"\"\"\n",
    "    elements = []\n",
    "    current_element = None\n",
    "    current_start = None\n",
    "    for i, ll in enumerate(l):\n",
    "        if ll == \"O\":\n",
    "            if current_element != None:\n",
    "                elements.append((current_element, current_start, i))\n",
    "            current_element = None\n",
    "            current_start = -1\n",
    "        elif ll[0] == \"B\":\n",
    "            if current_element != None:\n",
    "                elements.append((current_element, current_start, i))\n",
    "            current_element = ll[2:]\n",
    "            current_start = i\n",
    "        elif ll[0] == \"I\":\n",
    "            if current_element != ll[2:]:\n",
    "                elements.append((current_element, current_start, i))\n",
    "                current_element = ll[2:]\n",
    "                current_start = i\n",
    "    return elements\n",
    "\n",
    "\n",
    "def eval_ner(sys_labels, gold_labels):\n",
    "    \"\"\"\n",
    "    Computes precision, recall and f1-score, using get_ranges() to identify the named entities.\n",
    "    \"\"\"\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for sys_l, gold_l in zip(sys_labels, gold_labels):\n",
    "        sys_ranges = get_ranges(sys_l)\n",
    "        gold_ranges = get_ranges(gold_l)\n",
    "        for r in sys_ranges:\n",
    "            if r in gold_ranges:\n",
    "                tp += 1\n",
    "        ## TODO: also count fp and fn\n",
    "\n",
    "    recall = 0\n",
    "    prec = 0\n",
    "    f1score = 0\n",
    "    ## TODO: compute recall, precision and f1-score from tp, fp and fn\n",
    "\n",
    "    return recall, prec, f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w6KkbRcKuDe"
   },
   "source": [
    "Report precision, recall and f1-score of the model trained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCVkmVP1KuDe"
   },
   "source": [
    "**Task 2.4** (1 point): Train a new model with transition features and predict the development set labels using `predict_with_history`. How does this change affect the token-level (accuracy) and entity-level (recall, precision, f1-score) metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU25Jh0gKuDe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slkwmSUkKuDe"
   },
   "source": [
    "**Task 2.5** (2 points): Take some 50 sentences from the test set and display the words, gold tags, and the predicted tags of the two systems. Can you identify error patterns that are typical for the two systems respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6O_7gthcKuDe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRkZi7BLKuDe"
   },
   "source": [
    "## Part 3: Text classification with feed-forward neural networks\n",
    "\n",
    "For this part, we go back to the text classification task used in the first assignment: language identification of Bokmål and Nynorsk.\n",
    "\n",
    "Instead of manually creating a bag-of-subwords matrix with the BPE-encoded data, we go a simpler route this time and use the `CountVectorizer` class provided by Scikit-Learn. Have a look at the documentation of `CountVectorizer` here: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "With the default parameters, `CountVectorizer` will tokenize the data at whitespaces, remove punctuation signs and convert everything to lowercase. You can play around with the parameters if you wish, but the default settings work decently well. A useful thing to make the vectors smaller is to remove hapaxes, i.e. words that occur only once in the training data. You can achieve this with `min_df=2`.\n",
    "\n",
    "**Task 3.1** (2 points): Write the code to produce `train_X, train_Y, test_X, test_Y`. Each `*_X` variable should contain a list of vectors produced by `CountVectorizer`. Each `*_Y` variable should contain a list of integers (1 for Nynorsk and 0 for Bokmål)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6MhhGBQKuDe"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS41mrRxKuDe"
   },
   "source": [
    "**Task 3.2** (1 point): Create a logistic regression classifier, train it on the training set, and predict the labels of the test set. Compute accuracy, precision and recall, as in the previous assignment. How do your results compare with the previous assignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBn9C6L-KuDe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX7LXI1BKuDf"
   },
   "source": [
    "**Task 3.3** (1 point): Let us use a simple feed-forward neural network classifier instead of logistic regression, but keep the input feature representation identical. Scikit-Learn provides a simple feed-forward classifier under the name `MLPClassifier`. The most important parameter is `hidden_layer_sizes`, which specifies the number and size of hidden layers. For example, `MLPClassifier(hidden_layer_sizes=(10, 5))` creates a classifier with two hidden layers, the first one with 10 neurons and the second one with 5 neurons. To start, keep the model simple and choose a single hidden layer with 10 neurons. How does this model perform in comparison with logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xd8KxLnPKuDf"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxWYGxNeKuDf"
   },
   "source": [
    "**Task 3.4** (1 point): Try to improve the results by adding more and/or larger hidden layers. You're not expected to work with more than 50 neurons in total, as this slows down the training process drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAqHjlYcKuDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMoAvPO4KuDf"
   },
   "source": [
    "Now, let us see if we can improve the input representation. Instead of a bag-of-words model created with `CountVectorizer`, we will obtain document embeddings from a pretrained Norwegian SentenceBert model (in particular, the [base model from the National Library of Norway](https://huggingface.co/NbAiLab/nb-sbert-base)). The following snippet shows how to load and use such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfaKynXOKuDf"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Uncomment the corresponding line:\n",
    "\n",
    "# model = SentenceTransformer('NbAiLab/nb-sbert-base') # if you use your own environment\n",
    "\n",
    "# model = SentenceTransformer(\"NbAiLab/nb-sbert-base\", cache_folder=\"/fp/projects01/ec403/hf_models\") # if you use Fox\n",
    "\n",
    "sentences = [\n",
    "    \"This is a Norwegian boy\",\n",
    "    \"Dette er en norsk gutt\",\n",
    "    \"Dette er ein norsk gut\",\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS0qz3cHKuDf"
   },
   "source": [
    "**Task 3.5** (1 point): Produce embeddings for the training and test sets and train a new `MLPClassifier` with these embeddings as input features. Can you further improve the evaluation scores?\n",
    "\n",
    "Note: Producing the embeddings and training the model will take longer than in previous exercises, around 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jynCfnzCKuDf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
